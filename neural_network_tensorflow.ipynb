{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessed_mnist import load_dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define one-hot-encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y):\n",
    "    \"\"\"\n",
    "    Generate one-hot-ecoding for the labels:\n",
    "    params: list of labels: y\n",
    "    returns: numpy array of encoded labels of size [?,num_classes]\n",
    "    \n",
    "    \"\"\"\n",
    "    num_samples = y.shape[0]\n",
    "    num_class = np.unique(y).shape[0]   \n",
    "    \n",
    "    y_encoded = np.zeros((num_samples,num_class))\n",
    "    y_encoded[np.arange(num_samples),y] = 1\n",
    "\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing one-hot-encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels without one hot encoding:  [0 1 2 3]\n",
      "labels after one hot encoding: \n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(0,4)\n",
    "print('labels without one hot encoding: ',x)\n",
    "x = one_hot_encoding(x)\n",
    "print('labels after one hot encoding: ')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create random mini batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_minibatch(X,Y,batch_size):\n",
    "    \"\"\"\n",
    "    creates random minibatches from the data of size batch_size each\n",
    "    params: numpy array X (input data), numpy array Y (labels) and int batch_size \n",
    "    returns: list of tuples (of size 2) index 1: X and index 2: Y \n",
    "    \n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "    \n",
    "    num_samples = X.shape[0]\n",
    "    \n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)  # generate random permutation of indices.\n",
    "    \n",
    "    minibatches = []\n",
    "    for idx in range(0,num_samples,batch_size):\n",
    "        batch_indices = indices[idx:idx+batch_size]\n",
    "\n",
    "        minibatches.append((X[batch_indices,] ,Y[batch_indices,]))\n",
    " \n",
    "    return minibatches    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing random_minibatch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(0,20)\n",
    "y = np.arange(0,20)\n",
    "\n",
    "batches = random_minibatch(x,y,5)\n",
    "error_flag = 0\n",
    "\n",
    "for batch in batches:\n",
    "    arr_x = batch[0]\n",
    "    arr_y = batch[1]\n",
    "    if not np.array_equal(arr_x,arr_y):\n",
    "       error_flag = 1\n",
    "\n",
    "if error_flag:\n",
    "    print('Test Failed!')\n",
    "else:\n",
    "    print('Test Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function defining model inputs for the tensorlfow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    \"\"\"\n",
    "    create inputs (placeholders) for the tensorflow graph\n",
    "    params: none\n",
    "    returns: 2 tensor objects used as inputs to tensorflow of shape [None, 28*28] and [None, 10]\n",
    "    \n",
    "    \"\"\"\n",
    "    X_in = tf.placeholder(dtype=\"float32\",shape=[None,28*28]) \n",
    "    Y_in = tf.placeholder(dtype=\"float32\",shape=[None,10])\n",
    "    \n",
    "    print(X_in.shape[0])\n",
    "    \n",
    "    return X_in,Y_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model input shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholder_8:0' shape=(?, 784) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_9:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function which initializes weights for the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    initialize weights for the networks.\n",
    "    params: none\n",
    "    returns: dictionary of weights for the neural network. <key> : name , <value> : variable tensor \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(3) # set random seed \n",
    "\n",
    "    neurons_layer_1 = 500\n",
    "    neurons_layer_2 = 500\n",
    "    neurons_layer_3 = 500\n",
    "    neurons_layer_4 = 10\n",
    "    \n",
    "\n",
    "    init_w = tf.contrib.layers.xavier_initializer()  # weight initializer\n",
    "    init_b = tf.constant_initializer(0.0)  # bias initializer \n",
    "    \n",
    "    w1 = tf.get_variable(\"W1\",[28*28,neurons_layer_1], initializer=init_w)\n",
    "    b1 =  tf.get_variable(\"b1\",neurons_layer_1,initializer=init_b)\n",
    "    \n",
    "    w2 = tf.get_variable(\"W2\",[neurons_layer_1,neurons_layer_2], initializer=init_w)\n",
    "    b2 =  tf.get_variable(\"b2\",neurons_layer_2,initializer=init_b)\n",
    "    \n",
    "    w3 = tf.get_variable(\"W3\",[neurons_layer_2,neurons_layer_3], initializer=init_w)\n",
    "    b3 =  tf.get_variable(\"b3\",neurons_layer_3,initializer=init_b)\n",
    "    \n",
    "    w4 = tf.get_variable(\"W4\",[neurons_layer_3,neurons_layer_4], initializer=init_w)\n",
    "    b4 =  tf.get_variable(\"b4\",neurons_layer_4,initializer=init_b)\n",
    "    \n",
    "    \n",
    "    parameters = {\"w1\" : w1, \"b1\" : b1,\n",
    "                  \"w2\" : w2, \"b2\" : b2,\n",
    "                  \"w3\" : w3, \"b3\" : b3,\n",
    "                  \"w4\" : w4, \"b4\" : b4}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if model parameters are initialized properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b1': <tf.Variable 'b1:0' shape=(500,) dtype=float32_ref>,\n",
       " 'b2': <tf.Variable 'b2:0' shape=(500,) dtype=float32_ref>,\n",
       " 'b3': <tf.Variable 'b3:0' shape=(500,) dtype=float32_ref>,\n",
       " 'b4': <tf.Variable 'b4:0' shape=(10,) dtype=float32_ref>,\n",
       " 'w1': <tf.Variable 'W1:0' shape=(784, 500) dtype=float32_ref>,\n",
       " 'w2': <tf.Variable 'W2:0' shape=(500, 500) dtype=float32_ref>,\n",
       " 'w3': <tf.Variable 'W3:0' shape=(500, 500) dtype=float32_ref>,\n",
       " 'w4': <tf.Variable 'W4:0' shape=(500, 10) dtype=float32_ref>}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "initialize_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization (Not used) Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(layer):\n",
    "    \"\"\"\n",
    "    (unused function)\n",
    "    Computes mean and variance for the layer inputs and outpus\n",
    "    params: layer inputs (tensor object)\n",
    "    returns: normalized layer inputs.\n",
    "    \"\"\"\n",
    "    mean,variance = tf.nn.moments(layer,0)\n",
    "    return tf.nn.batch_normalization(layer,mean,variance,offset=0.,scale=1.,variance_epsilon=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fucntion defining forward Propagation in the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_forward_prop(X,parameters,keep_prob):\n",
    "    \"\"\"\n",
    "    feed forward 4-layer mlp architecture \n",
    "    params: placeholder: X and Variables: parameters\n",
    "    returns: tensor object (Dense): out\n",
    "    \"\"\"\n",
    "    w1 , b1  = parameters[\"w1\"] , parameters[\"b1\"]\n",
    "    w2 , b2  = parameters[\"w2\"] , parameters[\"b2\"]\n",
    "    w3 , b3  = parameters[\"w3\"] , parameters[\"b3\"]\n",
    "    w4 , b4  = parameters[\"w4\"] , parameters[\"b4\"]\n",
    "    \n",
    "    layer_1  = tf.matmul(X,w1) + b1\n",
    "    activation_1 = tf.nn.relu(layer_1) \n",
    "    dropout_1 = tf.nn.dropout(activation_1,keep_prob)\n",
    "    \n",
    "    layer_2  = tf.matmul(dropout_1,w2) + b2\n",
    "    activation_2 = tf.nn.relu(layer_2) \n",
    "    dropout_2 = tf.nn.dropout(activation_2,keep_prob)\n",
    "    \n",
    "    layer_3  = tf.matmul(dropout_2,w3) + b3\n",
    "    activation_3 = tf.nn.relu(layer_3)\n",
    "    dropout_3 = tf.nn.dropout(activation_3,keep_prob)\n",
    "    \n",
    "    out =  tf.matmul(dropout_3,w4) + b4\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function defining softmax cross entropy loss for the neural network model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y,pred,parameters):\n",
    "    \"\"\"\n",
    "    softmax cross entropy loss with l2 regularization \n",
    "    params: placeholder: y (labels) and pred (predicition) and Variable: weights. \n",
    "    returns: loss tensor object\n",
    "    \"\"\"\n",
    "    beta = 0.000005\n",
    "    regularizer = (beta * tf.nn.l2_loss(parameters[\"w1\"]) + \n",
    "                   beta * tf.nn.l2_loss(parameters[\"w2\"]) +\n",
    "                   beta * tf.nn.l2_loss(parameters[\"w3\"]) +\n",
    "                   beta * tf.nn.l2_loss(parameters[\"w4\"]))\n",
    "    \n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=pred)) + regularizer  # softmax ce loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocessing\n",
    "#### 1) Load the data\n",
    "#### 2) convert labels to one-hot-encoded vectors\n",
    "#### 3) flatten the MNIST Features\n",
    "#### 4) combine features and labels for train,test and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset() # load data\n",
    "\n",
    "# convert labels to one hot encodings\n",
    "y_train = one_hot_encoding(y_train) \n",
    "y_test = one_hot_encoding(y_test) \n",
    "y_val = one_hot_encoding(y_val) \n",
    "\n",
    "# flatten to 1D array from shape [num_samples,28,28]  to [num_samples,28*28]  \n",
    "X_train = np.reshape(X_train,(X_train.shape[0],28*28)) \n",
    "X_test = np.reshape(X_test,(X_test.shape[0],28*28)) \n",
    "X_val = np.reshape(X_val,(X_val.shape[0],28*28)) \n",
    "\n",
    "# group labels and features to form tuple of size 2.\n",
    "train_data = (X_train,y_train)\n",
    "test_data = (X_test,y_test)\n",
    "val_data = (X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and stop the training after 15 epochs or when validation accuracy above 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(train_data,val_data,num_epochs=20,learning_rate=0.001,minibatch_size=500):\n",
    "    \"\"\"\n",
    "    \n",
    "    Training the MLP until a fixed set of interations.\n",
    "    \n",
    "    params: training data: tuple of X [?,784] and Y [?,10]\n",
    "            validation data: tuple of X [?,784] and Y [?,10]\n",
    "            number of epochs (max): int   \n",
    "            learning rate (alpha): float\n",
    "            minibatch size: int\n",
    "    returns: val_loss_list: List of loss for validation set for training period (<=num_epochs)\n",
    "             train_loss_list: List of loss for training set for training period (<=num_epochs)\n",
    "             parameters: learned weights for the network (Variable tensor object)\n",
    "             sess: current session object. \n",
    "    \n",
    "    \"\"\"\n",
    "    tf.reset_default_graph() # reset\n",
    "    \n",
    "    X_train , y_train = train_data\n",
    "    X_val , y_val = val_data\n",
    "    \n",
    "    X_in,y_in = create_model_inputs()   # define placeholders\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    parameters = initialize_parameters() # define model parameters\n",
    "    \n",
    "    prediction  = nn_forward_prop(X_in,parameters,keep_prob) # feed-forward\n",
    "    \n",
    "    loss = compute_loss(y_in,prediction,parameters)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss) # back-prop\n",
    "    \n",
    "    # accuracy prediction\n",
    "    label_predictions = tf.argmax(prediction,1)\n",
    "    true_predictions = tf.equal(label_predictions,tf.argmax(y_in,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(true_predictions,\"float\"))\n",
    "    \n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    \n",
    "    # create and initialize variables through tensorflow session object.\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training for num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "            \n",
    "        train_loss = 0            \n",
    "        minibatches = random_minibatch(X_train,y_train,minibatch_size)\n",
    "        num_minibatches = int (X_train.shape[0] / minibatch_size)\n",
    "        \n",
    "        # Train for minibatch    \n",
    "        for minibatch in minibatches:\n",
    "                \n",
    "            X_minibatch , y_minibatch = minibatch\n",
    "                \n",
    "            _, minibatch_loss = sess.run([optimizer, loss], feed_dict={X_in:X_minibatch,y_in:y_minibatch,keep_prob:0.95}) # feed inputs \n",
    "                   \n",
    "            train_loss += minibatch_loss / num_minibatches \n",
    "            \n",
    "        val_loss,val_acc = sess.run([loss,accuracy], feed_dict={X_in:X_val,y_in:y_val,keep_prob:1.0})\n",
    "        train_acc = sess.run(accuracy,feed_dict={X_in:X_train,y_in:y_train,keep_prob:1.0})\n",
    "            \n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "            \n",
    "        print(('epoch %i: train_loss: %f  train_acc: %f, val_loss: %f val_acc: %f' \n",
    "               %(epoch+1,train_loss,train_acc,val_loss,val_acc)))\n",
    "        \n",
    "        # stop training when validation accuracy >=0.98\n",
    "        if val_acc >= 0.98:\n",
    "            break\n",
    "         \n",
    "    return train_loss_list,val_loss_list,parameters,sess\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Model Training *****\n",
      "Training for ( <= 20 epochs) \n",
      "?\n",
      "epoch 1: train_loss: 0.372399  train_acc: 0.960720, val_loss: 0.143368 val_acc: 0.959400\n",
      "epoch 2: train_loss: 0.125162  train_acc: 0.975360, val_loss: 0.110832 val_acc: 0.967500\n",
      "epoch 3: train_loss: 0.081243  train_acc: 0.986240, val_loss: 0.087305 val_acc: 0.973800\n",
      "epoch 4: train_loss: 0.058879  train_acc: 0.986160, val_loss: 0.089535 val_acc: 0.976200\n",
      "epoch 5: train_loss: 0.046323  train_acc: 0.989360, val_loss: 0.095963 val_acc: 0.974100\n",
      "epoch 6: train_loss: 0.039634  train_acc: 0.990640, val_loss: 0.101022 val_acc: 0.973600\n",
      "epoch 7: train_loss: 0.035679  train_acc: 0.993820, val_loss: 0.087996 val_acc: 0.976400\n",
      "epoch 8: train_loss: 0.027629  train_acc: 0.995260, val_loss: 0.086178 val_acc: 0.978900\n",
      "epoch 9: train_loss: 0.020646  train_acc: 0.996700, val_loss: 0.084287 val_acc: 0.980600\n",
      "\n",
      "Time elapsed: 67.575850 seconds\n",
      "\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "print('\\n**** Model Training *****\\nTraining for ( <= 20 epochs) ')\n",
    "start = time.time()\n",
    "train_loss,val_loss,parameters,sess = model_train(train_data,val_data)\n",
    "end = time.time()\n",
    "print('\\nTime elapsed: %f seconds'%(end-start))\n",
    "print('\\nTraining completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot training and validation loss with respect to the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss,val_loss):\n",
    "    \"\"\"\n",
    "    Plot of training and validation loss.\n",
    "    params:  val_loss_list: List of loss for validation set for training period (<=num_epochs)\n",
    "             train_loss_list: List of loss for training set for training period (<=num_epochs)\n",
    "    returns: None\n",
    "\n",
    "    \"\"\"\n",
    "    plt.plot(np.squeeze(train_loss),label='training')\n",
    "    plt.plot(np.squeeze(val_loss),label='validation')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Model Performance')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1968253def0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(train_loss,val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the performance of the model on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(test_data,best_parameters,sess):\n",
    "    \"\"\"\n",
    "    Model evalution for the test set,\n",
    "    params:  sess: default tensorflow session object.\n",
    "             test_data: tuple of X (features) and y(labels) \n",
    "             best_parameters: final model parameters (weights).\n",
    "    returns: Accuracy on the test data.\n",
    "\n",
    "    \"\"\"\n",
    "    X_test, y_test = test_data\n",
    "\n",
    "    X_in,y_in = create_model_inputs()\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    prediction  = nn_forward_prop(X_in,best_parameters,keep_prob)\n",
    "\n",
    "    \n",
    "    label_predictions = tf.argmax(prediction,1)\n",
    "    true_predictions = tf.equal(label_predictions,tf.argmax(y_in,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(true_predictions,\"float\"))\n",
    "    \n",
    "\n",
    "    acc = sess.run(accuracy, feed_dict={X_in:X_test,y_in:y_test,keep_prob:1.0})\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Model testing *****\n",
      "?\n",
      "Accuracy on the test set: 0.983100\n"
     ]
    }
   ],
   "source": [
    "print('\\n**** Model testing *****')\n",
    "acc = model_test(test_data,parameters,sess)\n",
    "print('Accuracy on the test set: %f' %(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
