{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessed_mnist import load_dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define one-hot-encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y):\n",
    "    \"\"\"\n",
    "    Generate one-hot-ecoding for the labels:\n",
    "    params: list of labels: y\n",
    "    returns: numpy array of encoded labels of size [?,num_classes]\n",
    "    \n",
    "    \"\"\"\n",
    "    num_samples = y.shape[0]\n",
    "    num_class = np.unique(y).shape[0]   \n",
    "    \n",
    "    y_encoded = np.zeros((num_samples,num_class))\n",
    "    y_encoded[np.arange(num_samples),y] = 1\n",
    "\n",
    "    return y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing one-hot-encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels without one hot encoding:  [0 1 2 3]\n",
      "labels after one hot encoding: \n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(0,4)\n",
    "print('labels without one hot encoding: ',x)\n",
    "x = one_hot_encoding(x)\n",
    "print('labels after one hot encoding: ')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create random mini batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_minibatch(X,Y,batch_size):\n",
    "    \"\"\"\n",
    "    creates random minibatches from the data of size batch_size each\n",
    "    params: numpy array X (input data), numpy array Y (labels) and int batch_size \n",
    "    returns: list of tuples (of size 2) index 1: X and index 2: Y \n",
    "    \n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "    \n",
    "    num_samples = X.shape[0]\n",
    "    \n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)  # generate random permutation of indices.\n",
    "    \n",
    "    minibatches = []\n",
    "    for idx in range(0,num_samples,batch_size):\n",
    "        batch_indices = indices[idx:idx+batch_size]\n",
    "\n",
    "        minibatches.append((X[batch_indices,] ,Y[batch_indices,]))\n",
    " \n",
    "    return minibatches    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing random_minibatch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(0,20)\n",
    "y = np.arange(0,20)\n",
    "\n",
    "batches = random_minibatch(x,y,5)\n",
    "error_flag = 0\n",
    "\n",
    "for batch in batches:\n",
    "    arr_x = batch[0]\n",
    "    arr_y = batch[1]\n",
    "    if not np.array_equal(arr_x,arr_y):\n",
    "       error_flag = 1\n",
    "\n",
    "if error_flag:\n",
    "    print('Test Failed!')\n",
    "else:\n",
    "    print('Test Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function defining model inputs for the tensorlfow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    \"\"\"\n",
    "    create inputs (placeholders) for the tensorflow graph\n",
    "    params: none\n",
    "    returns: 2 tensor objects used as inputs to tensorflow of shape [None, 28*28] and [None, 10]\n",
    "    \n",
    "    \"\"\"\n",
    "    X_in = tf.placeholder(dtype=\"float32\",shape=[None,28*28]) \n",
    "    Y_in = tf.placeholder(dtype=\"float32\",shape=[None,10])\n",
    "    \n",
    "    print(X_in.shape[0])\n",
    "    \n",
    "    return X_in,Y_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model input shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholder:0' shape=(?, 784) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_1:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function which initializes weights for the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    initialize weights for the networks.\n",
    "    params: none\n",
    "    returns: dictionary of weights for the neural network. <key> : name , <value> : variable tensor \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(3) # set random seed \n",
    "\n",
    "    neurons_layer_1 = 500\n",
    "    neurons_layer_2 = 500\n",
    "    neurons_layer_3 = 500\n",
    "    neurons_layer_4 = 10\n",
    "    \n",
    "\n",
    "    init_w = tf.contrib.layers.xavier_initializer()  # weight initializer\n",
    "    init_b = tf.constant_initializer(0.0)  # bias initializer \n",
    "    \n",
    "    w1 = tf.get_variable(\"W1\",[28*28,neurons_layer_1], initializer=init_w)\n",
    "    b1 =  tf.get_variable(\"b1\",neurons_layer_1,initializer=init_b)\n",
    "    \n",
    "    w2 = tf.get_variable(\"W2\",[neurons_layer_1,neurons_layer_2], initializer=init_w)\n",
    "    b2 =  tf.get_variable(\"b2\",neurons_layer_2,initializer=init_b)\n",
    "    \n",
    "    w3 = tf.get_variable(\"W3\",[neurons_layer_2,neurons_layer_3], initializer=init_w)\n",
    "    b3 =  tf.get_variable(\"b3\",neurons_layer_3,initializer=init_b)\n",
    "    \n",
    "    w4 = tf.get_variable(\"W4\",[neurons_layer_3,neurons_layer_4], initializer=init_w)\n",
    "    b4 =  tf.get_variable(\"b4\",neurons_layer_4,initializer=init_b)\n",
    "    \n",
    "    \n",
    "    parameters = {\"w1\" : w1, \"b1\" : b1,\n",
    "                  \"w2\" : w2, \"b2\" : b2,\n",
    "                  \"w3\" : w3, \"b3\" : b3,\n",
    "                  \"w4\" : w4, \"b4\" : b4}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if model parameters are initialized properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b1': <tf.Variable 'b1:0' shape=(500,) dtype=float32_ref>,\n",
       " 'b2': <tf.Variable 'b2:0' shape=(500,) dtype=float32_ref>,\n",
       " 'b3': <tf.Variable 'b3:0' shape=(500,) dtype=float32_ref>,\n",
       " 'b4': <tf.Variable 'b4:0' shape=(10,) dtype=float32_ref>,\n",
       " 'w1': <tf.Variable 'W1:0' shape=(784, 500) dtype=float32_ref>,\n",
       " 'w2': <tf.Variable 'W2:0' shape=(500, 500) dtype=float32_ref>,\n",
       " 'w3': <tf.Variable 'W3:0' shape=(500, 500) dtype=float32_ref>,\n",
       " 'w4': <tf.Variable 'W4:0' shape=(500, 10) dtype=float32_ref>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialize_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization (Not used) Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(layer):\n",
    "    \"\"\"\n",
    "    (unused function)\n",
    "    Computes mean and variance for the layer inputs and outpus\n",
    "    params: layer inputs (tensor object)\n",
    "    returns: normalized layer inputs.\n",
    "    \"\"\"\n",
    "    mean,variance = tf.nn.moments(layer,0)\n",
    "    return tf.nn.batch_normalization(layer,mean,variance,offset=0.,scale=1.,variance_epsilon=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fucntion defining forward Propagation in the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_forward_prop(X,parameters):\n",
    "    \"\"\"\n",
    "    feed forward 4-layer mlp architecture \n",
    "    params: placeholder: X and Variables: parameters\n",
    "    returns: tensor object (Dense): out\n",
    "    \"\"\"\n",
    "    w1 , b1  = parameters[\"w1\"] , parameters[\"b1\"]\n",
    "    w2 , b2  = parameters[\"w2\"] , parameters[\"b2\"]\n",
    "    w3 , b3  = parameters[\"w3\"] , parameters[\"b3\"]\n",
    "    w4 , b4  = parameters[\"w4\"] , parameters[\"b4\"]\n",
    "    \n",
    "    layer_1  = tf.matmul(X,w1) + b1\n",
    "    activation_1 = tf.nn.relu(layer_1) \n",
    "    dropout_1 = tf.nn.dropout(activation_1,0.95)\n",
    "    \n",
    "    layer_2  = tf.matmul(dropout_1,w2) + b2\n",
    "    activation_2 = tf.nn.relu(layer_2) \n",
    "    dropout_2 = tf.nn.dropout(activation_2,0.95)\n",
    "    \n",
    "    layer_3  = tf.matmul(dropout_2,w3) + b3\n",
    "    activation_3 = tf.nn.relu(layer_3)\n",
    "    dropout_3 = tf.nn.dropout(activation_3,0.85)\n",
    "    \n",
    "    out =  tf.matmul(dropout_3,w4) + b4\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function defining softmax cross entropy loss for the neural network model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y,pred,parameters):\n",
    "    \"\"\"\n",
    "    softmax cross entropy loss with l2 regularization \n",
    "    params: placeholder: y (labels) and pred (predicition) and Variable: weights. \n",
    "    returns: loss tensor object\n",
    "    \"\"\"\n",
    "    beta = 0.000005\n",
    "    regularizer = (beta * tf.nn.l2_loss(parameters[\"w1\"]) + \n",
    "                   beta * tf.nn.l2_loss(parameters[\"w2\"]) +\n",
    "                   beta * tf.nn.l2_loss(parameters[\"w3\"]) +\n",
    "                   beta * tf.nn.l2_loss(parameters[\"w4\"]))\n",
    "    \n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=pred)) + regularizer  # softmax ce loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocessing\n",
    "#### 1) Load the data\n",
    "#### 2) convert labels to one-hot-encoded vectors\n",
    "#### 3) flatten the MNIST Features\n",
    "#### 4) combine features and labels for train,test and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11419648/11490434 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset() # load data\n",
    "\n",
    "# convert labels to one hot encodings\n",
    "y_train = one_hot_encoding(y_train) \n",
    "y_test = one_hot_encoding(y_test) \n",
    "y_val = one_hot_encoding(y_val) \n",
    "\n",
    "# flatten to 1D array from shape [num_samples,28,28]  to [num_samples,28*28]  \n",
    "X_train = np.reshape(X_train,(X_train.shape[0],28*28)) \n",
    "X_test = np.reshape(X_test,(X_test.shape[0],28*28)) \n",
    "X_val = np.reshape(X_val,(X_val.shape[0],28*28)) \n",
    "\n",
    "# group labels and features to form tuple of size 2.\n",
    "train_data = (X_train,y_train)\n",
    "test_data = (X_test,y_test)\n",
    "val_data = (X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and stop the training after 15 epochs or when validation accuracy above 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(train_data,val_data,num_epochs=15,learning_rate=0.001,minibatch_size=500):\n",
    "    \"\"\"\n",
    "    \n",
    "    Training the MLP until a fixed set of interations.\n",
    "    \n",
    "    params: training data: tuple of X [?,784] and Y [?,10]\n",
    "            validation data: tuple of X [?,784] and Y [?,10]\n",
    "            number of epochs (max): int   \n",
    "            learning rate (alpha): float\n",
    "            minibatch size: int\n",
    "    returns: val_loss_list: List of loss for validation set for training period (<=num_epochs)\n",
    "             train_loss_list: List of loss for training set for training period (<=num_epochs)\n",
    "             parameters: learned weights for the network (Variable tensor object)\n",
    "             sess: current session object. \n",
    "    \n",
    "    \"\"\"\n",
    "    tf.reset_default_graph() # reset\n",
    "    \n",
    "    X_train , y_train = train_data\n",
    "    X_val , y_val = val_data\n",
    "    \n",
    "    X_in,y_in = create_model_inputs()   # define placeholders\n",
    "    parameters = initialize_parameters() # define model parameters\n",
    "    \n",
    "    prediction  = nn_forward_prop(X_in,parameters) # feed-forward\n",
    "    \n",
    "    loss = compute_loss(y_in,prediction,parameters)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss) # back-prop\n",
    "    \n",
    "    # accuracy prediction\n",
    "    label_predictions = tf.argmax(prediction,1)\n",
    "    true_predictions = tf.equal(label_predictions,tf.argmax(y_in,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(true_predictions,\"float\"))\n",
    "    \n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    \n",
    "    # create and initialize variables through tensorflow session object.\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training for num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "            \n",
    "        train_loss = 0            \n",
    "        minibatches = random_minibatch(X_train,y_train,minibatch_size)\n",
    "        num_minibatches = int (X_train.shape[0] / minibatch_size)\n",
    "        \n",
    "        # Train for minibatch    \n",
    "        for minibatch in minibatches:\n",
    "                \n",
    "            X_minibatch , y_minibatch = minibatch\n",
    "                \n",
    "            _, minibatch_loss = sess.run([optimizer, loss], feed_dict={X_in:X_minibatch,y_in:y_minibatch}) # feed inputs \n",
    "                   \n",
    "            train_loss += minibatch_loss / num_minibatches \n",
    "            \n",
    "        val_loss,val_acc = sess.run([loss,accuracy], feed_dict={X_in:X_val,y_in:y_val})\n",
    "        train_acc = sess.run(accuracy,feed_dict={X_in:X_train,y_in:y_train})\n",
    "            \n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "            \n",
    "        print(('epoch %i: train_loss: %f  train_acc: %f, val_loss: %f val_acc: %f' \n",
    "               %(epoch+1,train_loss,train_acc,val_loss,val_acc)))\n",
    "        \n",
    "        # stop training when validation accuracy >=0.98\n",
    "        if val_acc >= 0.98:\n",
    "            break\n",
    "         \n",
    "    return train_loss_list,val_loss_list,parameters,sess\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Model Training *****\n",
      "Training for ( <= 15 epochs) \n",
      "?\n",
      "epoch 1: train_loss: 0.377298  train_acc: 0.956560, val_loss: 0.154086 val_acc: 0.955400\n",
      "epoch 2: train_loss: 0.125280  train_acc: 0.972340, val_loss: 0.121775 val_acc: 0.963900\n",
      "epoch 3: train_loss: 0.080358  train_acc: 0.982340, val_loss: 0.101346 val_acc: 0.970800\n",
      "epoch 4: train_loss: 0.058987  train_acc: 0.984800, val_loss: 0.100538 val_acc: 0.972700\n",
      "epoch 5: train_loss: 0.048911  train_acc: 0.986380, val_loss: 0.109255 val_acc: 0.971200\n",
      "epoch 6: train_loss: 0.039831  train_acc: 0.988600, val_loss: 0.105905 val_acc: 0.974400\n",
      "epoch 7: train_loss: 0.035197  train_acc: 0.991440, val_loss: 0.102711 val_acc: 0.974000\n",
      "epoch 8: train_loss: 0.029042  train_acc: 0.994980, val_loss: 0.091099 val_acc: 0.976900\n",
      "epoch 9: train_loss: 0.020247  train_acc: 0.996740, val_loss: 0.091621 val_acc: 0.979200\n",
      "epoch 10: train_loss: 0.016599  train_acc: 0.995780, val_loss: 0.098673 val_acc: 0.977800\n"
     ]
    }
   ],
   "source": [
    "print('\\n**** Model Training *****\\nTraining for ( <= 15 epochs) ')\n",
    "start = time.time()\n",
    "train_loss,val_loss,parameters,sess = model_train(train_data,val_data)\n",
    "end = time.time()\n",
    "print('\\nTime elapsed: %f seconds'%(end-start))\n",
    "print('\\nTraining completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot training and validation loss with respect to the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss,val_loss):\n",
    "    \"\"\"\n",
    "    Plot of training and validation loss.\n",
    "    params:  val_loss_list: List of loss for validation set for training period (<=num_epochs)\n",
    "             train_loss_list: List of loss for training set for training period (<=num_epochs)\n",
    "    returns: None\n",
    "\n",
    "    \"\"\"\n",
    "    plt.plot(np.squeeze(train_loss),label='training')\n",
    "    plt.plot(np.squeeze(val_loss),label='validation')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Model Performance')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_loss,val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the performance of the model on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(test_data,best_parameters,sess):\n",
    "    \"\"\"\n",
    "    Model evalution for the test set,\n",
    "    params:  sess: default tensorflow session object.\n",
    "             test_data: tuple of X (features) and y(labels) \n",
    "             best_parameters: final model parameters (weights).\n",
    "    returns: Accuracy on the test data.\n",
    "\n",
    "    \"\"\"\n",
    "    X_test, y_test = test_data\n",
    "\n",
    "    X_in,y_in = create_model_inputs()\n",
    "    prediction  = nn_forward_prop(X_in,best_parameters)\n",
    "\n",
    "    \n",
    "    label_predictions = tf.argmax(prediction,1)\n",
    "    true_predictions = tf.equal(label_predictions,tf.argmax(y_in,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(true_predictions,\"float\"))\n",
    "    \n",
    "\n",
    "    acc = sess.run(accuracy, feed_dict={X_in:X_test,y_in:y_test})\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n**** Model testing *****')\n",
    "acc = model_test(test_data,parameters,sess)\n",
    "print('Accuracy on the test set: %f' %(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
